{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                Acer_Opalus\n",
      "1      Pterocarya_Stenoptera\n",
      "2       Quercus_Hartwissiana\n",
      "3            Tilia_Tomentosa\n",
      "4         Quercus_Variabilis\n",
      "               ...          \n",
      "985     Magnolia_Salicifolia\n",
      "986              Acer_Pictum\n",
      "987       Alnus_Maximowiczii\n",
      "988            Quercus_Rubra\n",
      "989           Quercus_Afares\n",
      "Name: species, Length: 990, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "print(train_df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train_df['id'].tolist()\n",
    "train_labels = train_df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images/952.jpg',\n",
       " 'images/900.jpg',\n",
       " 'images/1083.jpg',\n",
       " 'images/582.jpg',\n",
       " 'images/340.jpg',\n",
       " 'images/570.jpg',\n",
       " 'images/937.jpg',\n",
       " 'images/642.jpg',\n",
       " 'images/1523.jpg',\n",
       " 'images/201.jpg']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing the images\n",
    "image_directory = 'images'\n",
    "\n",
    "# List to store the image_list\n",
    "image_list = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(image_directory):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        label = int(os.path.splitext(filename)[0])  # Get the filename without the extension\n",
    "        if label in train_ids:\n",
    "            image_list.append(f\"images/{label}.jpg\")  # Convert to integer and add to the list\n",
    "len(image_list)\n",
    "image_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48840 1385520\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=185x264 at 0x7F6B0C473090> <PIL.JpegImagePlugin.JpegImageFile image mode=L size=1380x1004 at 0x7F6B0C558C10>\n"
     ]
    }
   ],
   "source": [
    "smallest_image = float('inf')\n",
    "largest_image = float('-inf')\n",
    "smallest_path = ''\n",
    "largest_path = ''\n",
    "image_width = []\n",
    "image_height = []\n",
    "images = []\n",
    "for image_path in image_list:\n",
    "    image = Image.open(f\"{image_path}\")\n",
    "    image_width.append(image.size[0])\n",
    "    image_height.append(image.size[1])\n",
    "    images.append(image)\n",
    "    image_dims = image.size[0]*image.size[1]\n",
    "    if image_dims < smallest_image:\n",
    "        smallest_image = image_dims\n",
    "        smallest_path = image\n",
    "    elif image_dims > largest_image:\n",
    "        largest_image = image_dims\n",
    "        largest_path = image\n",
    "\n",
    "print(smallest_image, largest_image)\n",
    "print(smallest_path, largest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(684, 684)\n"
     ]
    }
   ],
   "source": [
    "median_width = median_height = max(int(np.median(image_width)), int(np.median(image_height))) \n",
    "resize_shape = (median_width, median_height)\n",
    "print(resize_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck2): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=99, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: 684x684x1, Output: 684x684x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 342x342x32\n",
    "        )\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 342x342x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 171x171x64\n",
    "        )\n",
    "\n",
    "        # Third convolutional block (bottleneck)\n",
    "        self.bottleneck1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0),  # Output: 171x171x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),   # Output: 171x171x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)   # Output: 171x171x128\n",
    "        )\n",
    "\n",
    "        # Fourth convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: 171x171x256\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 85x85x256\n",
    "        )\n",
    "\n",
    "        # Fifth convolutional block (bottleneck)\n",
    "        self.bottleneck2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0),  # Output: 85x85x512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),   # Output: 85x85x256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0)   # Output: 85x85x512\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling and classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Reduce to 1x1 feature map\n",
    "        self.fc = nn.Linear(512, num_classes)  # Final output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bottleneck1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bottleneck2(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "num_classes = 99  # Change this to your number of classes\n",
    "model = CustomCNN(num_classes=num_classes)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(resize_shape),  # Resizing to 64x64 during transformation\n",
    "    transforms.RandomRotation(10),  # 10 random rotations\n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal flipping for augmentation\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_labels = LabelEncoder().fit_transform(train_labels)\n",
    "dataset = CustomDataset(image_list, train_labels, transform=transform)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN(num_classes=99)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjusted learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 50  # Increased number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images, labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "torch.save(model.state_dict(), 'model1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
