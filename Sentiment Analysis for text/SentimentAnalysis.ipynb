{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by Rishabh Chauhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) task that involves determining the sentiment expressed in a piece of text. This project focuses on sentiment analysis using deep learning techniques to classify text data into different sentiment categories, such as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "\n",
    "The primary goal of this notebook is to build a sentiment analysis model that can automatically classify the sentiment of textual data. The model utilizes deep learning architectures, including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), to capture complex patterns and relationships within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "\n",
    "The sentiment analysis model is trained and evaluated on a dataset containing labeled examples of text with corresponding sentiment labels. The dataset consists of a variety of sentences from social media, customer reviews, or other sources, each annotated with its sentiment (positive, negative, or neutral).\n",
    "\n",
    "link to the dataset: https://www.kaggle.com/datasets/saurabhshahane/twitter-sentiment-dataset\n",
    "Acknowledgement: HUSSEIN, SHERIF (2021), “Twitter Sentiments Dataset”, Mendeley Data, V1, doi: 10.17632/z9zw7nt5h2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So below are all the libraries that you will need for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin with loading the dataset. The dataset does not have dedicated columns so first we will provide the column names as an argument in form of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Twitter_Data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"clean_text\": \"sentences\", \"category\":\"target\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step above is not necessary its just for my convenience!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will work with 3 sentiments i.e. 'Negetive', 'Neutral' 'Positive'. So lets filter the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       " 1.0    72250\n",
       " 0.0    55213\n",
       "-1.0    35510\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the code below sets up functions for common text preprocessing tasks, such as removing stopwords, handling user mentions and URLs, and lemmatizing words.\n",
    "\n",
    "These preprocessing steps are crucial for cleaning and transforming raw text data before using it for NLP tasks like sentiment analysis or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stop_words(sentence): \n",
    "  if isinstance(sentence, str):\n",
    "    words = sentence.split() \n",
    "    filtered_words = [word for word in words if word not in stop_words] \n",
    "    return ' '.join(filtered_words)\n",
    "  else:\n",
    "    return \"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    if isinstance(text, str):\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        st = \"\"\n",
    "        for w in w_tokenizer.tokenize(text):\n",
    "            st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "        return st.strip()\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi promised “minimum government maximum gove...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking supporter prefix chowkidar name modi gr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  target\n",
       "0  modi promised “minimum government maximum gove...    -1.0\n",
       "1             talk nonsense continue drama vote modi     0.0\n",
       "2  say vote modi welcome bjp told rahul main camp...     1.0\n",
       "3  asking supporter prefix chowkidar name modi gr...     1.0\n",
       "4  answer among powerful world leader today trump...     1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentences']=data['sentences'].apply(preprocess)\n",
    "data['sentences']=data['sentences'].apply(remove_stop_words)\n",
    "data['sentences']=data['sentences'].apply(lemmatize_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to calculate the average length of each sentence in the dataset (This will be of help later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.27799116456006\n"
     ]
    }
   ],
   "source": [
    "def Cavg(text):\n",
    "    arr=[]\n",
    "    for i in text:\n",
    "        np.array(arr.append(len(i)))\n",
    "    return(np.average(arr))\n",
    "avg_len=Cavg(data['sentences'])\n",
    "print(avg_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up a tokenizer, fits it on the text data, and converts the text into sequences of integers. This sequence data is often used as input to train machine learning models, such as neural networks, for tasks like sentiment analysis or text classification. The choice of num_words limits the vocabulary size to the most frequent words, which can help manage computational resources and improve model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word=5000 #the maximum number of words to keep, based on word frequency.\n",
    "max_sequence_length = 100 #the average length was 96 so we can round of to 100\n",
    "tokenizer = Tokenizer(num_words=max_word, split=' ') \n",
    "tokenizer.fit_on_texts(data['sentences'].values)\n",
    "X = tokenizer.texts_to_sequences(data['sentences'].values)\n",
    "X= pad_sequences(X, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    1  247   27 1579 1016 1251 1092   50  105   41   14   25  970\n",
      "  105  405 3652 1103]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the preprocessing of the dataset, the next step is to start preparing for deep learning modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhchauhan/pyth/Machine-Learning-Projects/.venv/lib/python3.10/site-packages/pandas/core/series.py:953: RuntimeWarning: invalid value encountered in cast\n",
      "  arr = np.asarray(values, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "y = keras.utils.to_categorical(data['target'], 3, dtype=\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly take a look at how the data is looking after splitting for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114086, 100)\n",
      "(114086, 3)\n",
      "(48894, 100)\n",
      "(48894, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code give below defines a sequential neural network model for a binary classification task using Keras. It consists of an Embedding layer followed by two Bidirectional LSTM layers with dropout. The final layer is a Dense layer with softmax activation. The model is compiled using the Adam optimizer and categorical crossentropy loss, with accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_word, 100, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.5)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up callbacks for model training in Keras. It includes a ModelCheckpoint callback to save the best model based on validation loss and an EarlyStopping callback to halt training if the validation loss doesn't improve for a specified number of epochs. The model is then trained for 5 epochs (the higher the value higher will be the training cycles and higher the amount of time taken) using the provided training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n",
      "3566/3566 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.8664\n",
      "Epoch 1: val_loss improved from inf to 0.35320, saving model to best_model.hdf5\n",
      "3566/3566 [==============================] - 496s 139ms/step - loss: 0.4023 - accuracy: 0.8664 - val_loss: 0.3532 - val_accuracy: 0.8898\n",
      "Epoch 2/5\n",
      "   1/3566 [..............................] - ETA: 7:14 - loss: 0.1719 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhchauhan/pyth/Machine-Learning-Projects/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3566/3566 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8912\n",
      "Epoch 2: val_loss improved from 0.35320 to 0.34191, saving model to best_model.hdf5\n",
      "3566/3566 [==============================] - 495s 139ms/step - loss: 0.3391 - accuracy: 0.8912 - val_loss: 0.3419 - val_accuracy: 0.8941\n",
      "Epoch 3/5\n",
      "3566/3566 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8961\n",
      "Epoch 3: val_loss did not improve from 0.34191\n",
      "3566/3566 [==============================] - 489s 137ms/step - loss: 0.3198 - accuracy: 0.8961 - val_loss: 0.3438 - val_accuracy: 0.8937\n",
      "Epoch 4/5\n",
      "3566/3566 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9001\n",
      "Epoch 4: val_loss did not improve from 0.34191\n",
      "3566/3566 [==============================] - 515s 144ms/step - loss: 0.3005 - accuracy: 0.9001 - val_loss: 0.3434 - val_accuracy: 0.8969\n",
      "Epoch 5/5\n",
      "3566/3566 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.9052\n",
      "Epoch 5: val_loss did not improve from 0.34191\n",
      "3566/3566 [==============================] - 494s 138ms/step - loss: 0.2817 - accuracy: 0.9052 - val_loss: 0.3562 - val_accuracy: 0.8948\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='auto', period=1, save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete we will test the model on testing dataset and generate the classification report to analyse the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528/1528 [==============================] - 55s 35ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91     16551\n",
      "           1       0.93      0.88      0.91     21626\n",
      "           2       0.88      0.79      0.83     10717\n",
      "\n",
      "    accuracy                           0.89     48894\n",
      "   macro avg       0.89      0.88      0.88     48894\n",
      "weighted avg       0.89      0.89      0.89     48894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "report = classification_report(y_test.argmax(axis=1), np.around(y_pred, decimals=0).argmax(axis=1))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets save the model and the tokenizer using \"model.save()\" so that it can be used later for applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving deep learning model\n",
    "model.save(\"SentModel.h5\")\n",
    "# saving tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the model and tokenizer we can load using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "loaded_model=keras.models.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100\n",
    "sentiment=['Neutral', 'Positive',\"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Negative\n",
      "Positive\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "test_txt=[\"I hate everything in my life\",\"I love what i saw last night\",\"hi how are you?\"]\n",
    "seq = tokenizer.texts_to_sequences(test_txt)\n",
    "ans = pad_sequences(seq, maxlen=max_sequence_length)\n",
    "preds = model.predict(ans)\n",
    "for i in preds:\n",
    "    print(sentiment[np.around(i, decimals=0).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to see one of my applications of sentiment analysis check out the following repository on my profile:\n",
    "\n",
    "https://github.com/Rc17git/YT_Comment_Analysis_Webbapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
